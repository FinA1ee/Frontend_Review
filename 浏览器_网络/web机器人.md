Web机器人：在无需人类干预的情况下自动进行一系列Web事务处理的软件程序。

## 爬虫 Crawler

对web网站进行遍历，获取web页面。搜索引擎使用爬虫搜集文档，形成可搜索的数据库。

- 寻找根集：根集需要能达到所有页面
- 避免环路：文件系统环路，恶意的虚拟web空间
- 储存访问过的URL的数据结构
  1. 树/散列表：记录已访问的URL
  2. 检查点：将已访问的URL列表即时保存在硬盘

### 爬虫如何避免环路？

- 规范化URL：避免别名
- 广度优先
- 节流：限制单个机器人可获取页面的数量
- URL/站点黑名单
- 页面内容指纹：检测一个页面是否被重复访问

### web机器人的HTTP如何配置？

- User-Agent首部：机器人的名字
- From首部：机器人管理者的Email地址
- Accept首部：告知服务器可以发送哪些媒体类型，让机器人接受其感兴趣的内容
- Host首部：防止机器人将错误的内容与默认URL关联起来 （例子：两个站点部署在一个服务器上）
- web管理者需要为非浏览器请求（机器人请求）开发特定的页面

### 如何处理行为不当的机器人？

- 机器人失控：导致陷入环路，服务器过载，需要设计保护措施
- 失效的URL：机器人大量访问失效URL，降低服务器处理能力
- 机器人可能会获得站点内的敏感数据
- web服务器根目录中的Robots.txt文件，可以设置机器人权限

### 搜索引擎

- 全文索引：关键字数据库，装载所有web页面和其内容

- 发布查询请求

- 结果排序：在爬行过程中进行数据统计

  

  



